# CVE-2025-2828 (LangChain RequestsToolkit) — SSRF via Unrestricted HTTP Tools → Internal Memory Poisoning → AI Output Manipulation (Lab Case)

## Overview

This lab case documents a controlled validation of **CVE-2025-2828** affecting **`langchain-community` RequestsToolkit**.

**Core issue (high level):**  
A developer can instantiate `RequestsToolkit(requests_wrapper=...)` and expose the generated **HTTP tools** (e.g., GET/POST) to an LLM/Agent. If the tool layer is not configured to restrict “dangerous requests” and does not enforce an internal IP allowlist/denylist, the agent may be coerced into making **server-side requests to localhost / internal networks** (SSRF), such as `127.0.0.1`.

**What we demonstrate (lab-only):**
1. **SSRF write primitive**: the agent is induced to send a POST request to an internal-only service on `127.0.0.1`.
2. **Internal data tampering**: a record is written into an internal “memory/decision DB” without the real user’s awareness.
3. **Model output manipulation (primary impact)**: a real user later asks a normal question; the assistant fetches the poisoned data and returns a **wrong business conclusion**.

> ⚠️ Responsible Use Only  
> This README is intended for **isolated lab validation** and **defensive verification**.  
> It keeps exploitation details **high-level** and uses examples as “markers” rather than reusable attack recipes.

---

## Why this is not prompt injection

This is not “prompt injection” in the classic sense. The primary weakness is **network reachability**: the agent is granted an HTTP capability that can reach **localhost/internal services**, and the tool layer does not sufficiently constrain where requests may go. Prompting is only the mechanism used to *trigger* the tool call.

---

## Scenario (Enterprise Assistant Pattern)

We simulate a common enterprise setup:

- **Architecture**: AI Agent (LangChain) + internal microservice (Flask “DB”)
- **Expected behavior**: user asks a business question → agent queries internal service → agent answers
- **Attack goal**: abuse SSRF to **write fake “decision history”** into the internal service
- **Final impact**: when a real user asks later, the AI answers based on the **tampered internal data**

---

## Components & Lab Setup

- **Vulnerable component**: `langchain-community` — `RequestsToolkit` (HTTP tools exposed to the agent)
- **Model**: Qwen2.5-7B (Ollama)
- **PoC files**:
  - `internal_memory_service.py` — internal Flask service (simulated “memory DB”)
  - `memory_attack.py` — agent + RequestsToolkit demonstration (injection + victim query)

---

## Lab Topology

- **Internal service (target)**: `http://127.0.0.1:9000`
  - `POST /add_history` — adds a record (includes “implicit user creation” logic)
  - `GET /get_history?user_id=...` — returns history records
- **Agent runtime**: runs on the same host; therefore, tool calls can reach `127.0.0.1`
- **Attacker**: provides natural-language input to the agent to induce tool calls
- **Victim**: later asks an innocuous question (“What was my recent decision?”)

---

## End-to-End Flow (High Level)

### Phase A — Baseline
1. Start the internal service (empty DB / clean baseline).
2. Confirm the service is reachable only from the host/network context where the agent runs.

### Phase B — Injection (SSRF Write)
1. The attacker interacts with the agent.
2. The agent is induced to call the **POST HTTP tool** targeting `127.0.0.1:9000/add_history`.
3. The internal service accepts the request and stores a new “decision” record (poisoned data).

### Phase C — Victim Query (Normal UX, Poisoned Answer)
1. The victim asks a normal question in natural language (no URLs, no special syntax).
2. The agent calls the **GET HTTP tool** to fetch the victim’s decision history.
3. The model answers using the tampered record, producing an incorrect conclusion.

**Key point:** The victim does not need to do anything “unsafe.” The integrity failure comes from the agent’s ability to reach and modify internal services.

---

## Evidence (Screenshots)

### Evidence A — Injection Phase (POST tool call + server confirmation)
- Agent logs show a successful **`requests_post`** call to:
  - `http://127.0.0.1:9000/add_history`
- The internal service confirms:
  - a record was added for the target user ID
  - HTTP 200 response returned

This proves: **SSRF reachability + write succeeded**.

![inject](assets\inject.png)

### Evidence B — Victim Phase (Natural-language query → poisoned answer)
- Victim input is only natural language (e.g., “What was my recent decision?”).
- Agent logs show a **`requests_get`** call to:
  - `http://127.0.0.1:9000/get_history?user_id=...`
- The assistant’s response includes the injected “decision” marker, confirming:
  - internal data was read and used
  - the final output reflects the tampered record

This proves: **data tampering reliably manipulates model output**.

![after](assets\after.png)

---

## Impact

- **SSRF risk**: agent-enabled HTTP tools can reach internal hosts/services (localhost, RFC1918 ranges, metadata endpoints, etc.).
- **Integrity compromise**: internal business data can be modified, causing downstream reasoning errors.
- **Trust failure**: users receive answers that appear legitimate because they are “based on internal sources,” but the sources were poisoned.

---

## Notes

This lab illustrates a common pattern: once an agent is granted “general HTTP power,” it can become an **unintended bridge** across network boundaries. Even if the model is aligned, attacker-controlled prompting can still coerce tool use unless the tool layer and network layer enforce strong constraints.